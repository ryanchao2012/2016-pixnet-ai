{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "from urllib import request\n",
    "import codecs, jieba\n",
    "import re, html, time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "\n",
    "jieba.add_word('龐傍燮謝', freq=10, tag='xx')\n",
    "\n",
    "class GoogleCrawler(object):\n",
    "    def __init__(self, qno, wlist, qidx, anslist):\n",
    "        self.qno = qno\n",
    "        self.wlist = wlist\n",
    "        self.iqlist = qidx\n",
    "        self.anslist = anslist\n",
    "        self.qnum = 2\n",
    "        self.rand_query = []\n",
    "        self.set_rand_query()\n",
    "    \n",
    "    def add_rand_query(self, rwlist = None, qidx = None):\n",
    "        if rwlist == None or qidx == None:\n",
    "            rwlist = self.wlist\n",
    "            qidx = self.iqlist\n",
    "            rwlen = len(rwlist)\n",
    "            \n",
    "            for i in range(self.qnum):\n",
    "                iq = random.choice(qidx)\n",
    "                if iq > 0:\n",
    "                    ihead = random.choice(range(0, iq))\n",
    "                else:\n",
    "                    ihead = 0\n",
    "                if iq + 1 < rwlen:\n",
    "                    iend = random.choice(range(iq + 1, rwlen))\n",
    "                else:\n",
    "                    iend = rwlen- 1\n",
    "                self.rand_query.append(''.join(rwlist[ihead : iend]))\n",
    "                \n",
    "    def set_rand_query(self):\n",
    "        self.rand_query = []\n",
    "        self.add_rand_query()\n",
    "    \n",
    "    def get_rand_query(self):\n",
    "        return self.rand_query\n",
    "    \n",
    "    def print_rand_query(self):\n",
    "        for q in self.rand_query:\n",
    "            print(q)\n",
    "    \n",
    "    def crawl(self, query):\n",
    "        raw_html = self.google_crawl(query).lower()\n",
    "        clean_html = self.clean_html(raw_html)\n",
    "        return clean_html\n",
    "    \n",
    "    def multitask_craw(self):\n",
    "        pass\n",
    "    \n",
    "    def fast_search(self):\n",
    "        html_pool = ''\n",
    "        for q in self.rand_query:\n",
    "            html_pool += self.crawl(q)\n",
    "            time.sleep(0.5)\n",
    "        for x in self.anslist:\n",
    "            if html_pool.find(x) > 0:\n",
    "                return (self.anslist.index(x), x)\n",
    "        \n",
    "    def google_crawl(self, query = None):\n",
    "        if query == None: query = self.short_query\n",
    "        self.link = \"https://www.google.com/search?q=\" + quote(query)  + '&ie=utf8&oe=utf8' # + '&lr=lang_zh-TW'\n",
    "        req = request.Request(self.link, headers = {'User-Agent' : \"Chrome Browser\"})\n",
    "        raw = html.unescape(request.urlopen(req).read().decode('utf-8'))\n",
    "        return raw\n",
    "    \n",
    "    def clean_html(self, raw_html = None):\n",
    "        if raw_html == None: raw_html = self.google_crawl(self.short_query)\n",
    "        clean_html = re.sub(re.compile(r'(<br?>)|(</br?>)|\\n|\\r|\\s'), '', raw_html)\n",
    "        return clean_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['因為', '地處', '*', '的', '高點', '-', '-', '號稱', '\"', '全', '*', '最高', '的', '溫泉', '\"', ',', '又', '有', '\"', '*', '101', '\"', '的', '別']\n",
      "[2, 10, 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, '烏來')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import random\n",
    "# question = ['*', '水利局', '副局長', '彭紹博', '表示','，', '該工程','是',\n",
    "#             '經濟部', '水利署', '所', '建', '，', '但', '因', '工程', '已經', '完工','，',\n",
    "#             '市府', '針對', '此','狀況','，', '正', '積極', '尋求','補救','，','希望','在',\n",
    "#             '兩岸', '闢建', '公園', '，', '*', '但', '因', '地價', '太高', '，', '還在', '努力中']\n",
    "# q_index = [0,35]\n",
    "\n",
    "# crawler = GoogleCrawler(question, q_index)\n",
    "# crawler.print_rand_query()\n",
    "sample = '''\\\n",
    "[1] 熱水池可以容納二大二小沒問題，果然到︽⊙＿⊙︽泡無色無味的溫泉很舒服，泡的身子暖呼呼，\\\n",
    "回民宿後果然很好睡呀 ### a: 聚會, b: 礁溪, c: 貝殼, d: 高昂, e: 開機 [END]'''\n",
    "\n",
    "jieba.add_word('龐傍燮謝', freq=10, tag='xx')\n",
    "\n",
    "def simple_preprocess(sample):\n",
    "    no, content, a, b, c, d, e = re.findall(r'\\[(\\d+)\\](.*)### a:(.*), b:(.*), c:(.*), d:(.*), e:(.*)\\[END\\]', sample)[0]\n",
    "    options = [a.strip(), b.strip(), c.strip(), d.strip(), e.strip()]\n",
    "    content = re.sub('(\")(.?)', r'\\2', content.strip())\n",
    "    content = content.strip().replace('︽⊙＿⊙︽', '龐傍燮謝')\n",
    "    wlist = list(jieba.cut(content))\n",
    "    qidx = []\n",
    "    i = 0\n",
    "    for w in wlist:\n",
    "        if w == '龐傍燮謝':\n",
    "            wlist[i] = '*'\n",
    "            qidx.append(i)\n",
    "        i += 1\n",
    "    return (no, wlist, qidx, options)\n",
    "\n",
    "path = 'question_samples/'\n",
    "filename = '2016-07-31-15-02-02.txt'\n",
    "no = 4\n",
    "def get_sample(file_name, no_sample):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            for (no, content, a, b, c, d, e) in re.findall(r'\\[(\\d+)\\](.*)### a:(.*), b:(.*), c:(.*), d:(.*), e:(.*)\\[END\\]', line):\n",
    "                if int(no) == int(no_sample):\n",
    "                    return line\n",
    "\n",
    "no, wlist, qidx, anslist = simple_preprocess(get_sample(path + filename, no))\n",
    "print(no)\n",
    "print(wlist)\n",
    "print(qidx)\n",
    "crawler = GoogleCrawler(no, wlist, qidx, anslist)\n",
    "crawler.fast_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "path = 'question_samples/'\n",
    "sample_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "i, j = 0, 0\n",
    "for file in sample_files:\n",
    "    with codecs.open(path + file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.find('︽⊙＿⊙︽') > 0:\n",
    "                no, wlist, qidx, anslist = simple_preprocess(line)\n",
    "                crawler = GoogleCrawler(no, wlist, qidx, anslist)\n",
    "                try:\n",
    "                    ans = crawler.fast_search()\n",
    "                    if ans == None:\n",
    "                        print('[failure]{}:{}'.format(file, line[:3]))\n",
    "                        j += 1\n",
    "                except:\n",
    "                    print('[exception]:{}:{}'.format(file, line[:3]))\n",
    "                    j += 1\n",
    "                i += 1\n",
    "                if i % 100 == 1: print(1. - float(j) / float(i))\n",
    "                time.sleep(0.5)\n",
    "print(1. - float(j) / float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1] 這裡之前gisele跟hannah都跟我大力推薦, 因為這個spa用的產品全部都是來自約旦死海的︽⊙＿⊙ \\t### a: 保養品, b: 陰雨, c: 柿餅, d: 冬粉, e: 問度 [END]\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line\n",
    "# print(crawler.wlist)\n",
    "# print(crawler.rand_query)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
