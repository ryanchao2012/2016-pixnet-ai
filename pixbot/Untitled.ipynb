{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "from urllib import request\n",
    "import codecs\n",
    "import re, html\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "class GoogleCrawler(object):\n",
    "    def __init__(self, qno, wlist, qidx, anslist):\n",
    "        self.qno = qno\n",
    "        self.wlist = wlist\n",
    "        self.iqlist = qidx\n",
    "        self.anslist = anslist\n",
    "        self.qnum = 2\n",
    "        self.rand_query = []\n",
    "        self.set_rand_query()\n",
    "    \n",
    "    def add_rand_query(self, rwlist = None, qidx = None):\n",
    "        if rwlist == None or qidx == None:\n",
    "            rwlist = self.wlist\n",
    "            qidx = self.iqlist\n",
    "            rwlen = len(rwlist)\n",
    "            \n",
    "            for i in range(self.qnum):\n",
    "                iq = random.choice(qidx)\n",
    "                if iq > 0:\n",
    "                    ihead = random.choice(range(0, iq))\n",
    "                else:\n",
    "                    ihead = 0\n",
    "                if iq + 1 < rwlen:\n",
    "                    iend = random.choice(range(iq + 1, rwlen))\n",
    "                else:\n",
    "                    iend = rwlen- 1\n",
    "                self.rand_query.append(''.join(rwlist[ihead : iend]))\n",
    "                \n",
    "    def set_rand_query(self):\n",
    "        self.rand_query = []\n",
    "        self.add_rand_query()\n",
    "    \n",
    "    def get_rand_query(self):\n",
    "        return self.rand_query\n",
    "    \n",
    "    def print_rand_query(self):\n",
    "        for q in self.rand_query:\n",
    "            print(q)\n",
    "    \n",
    "    def crawl(self, query):\n",
    "        raw_html = self.google_crawl(query).lower()\n",
    "        clean_html = self.clean_html(raw_html)\n",
    "        return clean_html\n",
    "    \n",
    "    def multitask_craw(self):\n",
    "        pass\n",
    "    \n",
    "    def fast_search(self):\n",
    "        html_pool = ''\n",
    "        for q in self.rand_query:\n",
    "            html_pool += self.crawl(q)   \n",
    "        for x in self.anslist:\n",
    "            if html_pool.find(x) > 0:\n",
    "                return (self.anslist.index(x), x)\n",
    "        \n",
    "    def google_crawl(self, query = None):\n",
    "        if query == None: query = self.short_query\n",
    "        self.link = \"https://www.google.com/search?q=\" + quote(query)  + '&ie=utf8&oe=utf8' # + '&lr=lang_zh-TW'\n",
    "        req = request.Request(self.link, headers = {'User-Agent' : \"Pix Browser\"})\n",
    "        raw = html.unescape(request.urlopen(req).read().decode('utf-8'))\n",
    "        return raw\n",
    "    \n",
    "    def clean_html(self, raw_html = None):\n",
    "        if raw_html == None: raw_html = self.google_crawl(self.short_query)\n",
    "        clean_html = re.sub(re.compile(r'(<br?>)|(</br?>)|\\n|\\r|\\s'), '', raw_html)\n",
    "        return clean_html\n",
    "    \n",
    "# class GoogleCrawler(object):\n",
    "#     def __init__(self, sample, max_padding = 18):\n",
    "#         for (no, content, a, b, c, d, e) in re.findall(r'\\[(\\d+)\\](.*)### a:(.*), b:(.*), c:(.*), d:(.*), e:(.*)\\[END\\]', sample):\n",
    "#             self.no = int(no)\n",
    "#             content = re.sub('(\")(.?)', r'\\2', content.strip())\n",
    "#             self.raw_query = content.replace('︽⊙＿⊙︽', '*')\n",
    "#             self.raw_pattern = re.sub(r'\\s+', '\\\\s?', content.replace('︽⊙＿⊙︽', '(.*?)'))\n",
    "#             self.options = [x.strip() for x in [a, b, c, d, e]]\n",
    "#             break\n",
    "#         self.max_padding = max_padding\n",
    "#         self.short_query = self.make_short_string(self.raw_query, '*')\n",
    "#         self.short_pattern = self.make_short_string(self.raw_pattern, '(.*?)')\n",
    "        \n",
    "#     def set_padding(self, padding):\n",
    "#         self.max_padding = padding\n",
    "#         self.short_query = self.make_short_string(self.raw_query, '*')\n",
    "#         self.short_pattern = self.make_short_string(self.raw_pattern, '(.*?)')\n",
    "\n",
    "#     def get_link(self, query = None):\n",
    "#         if query == None: query = self.short_query\n",
    "#         if self.link == None:\n",
    "#             self.link = \"https://www.google.com/search?q=\" + quote(query)\n",
    "# #             + '&lr=lang_zh-TW' + '&nfpr=1'\n",
    "#         return self.link\n",
    "\n",
    "#     def make_short_string(self, string, pattern):\n",
    "#         pattern_len = len(pattern)\n",
    "#         string_len = len(string)\n",
    "#         idx = string.find(pattern)\n",
    "#         left_bound = idx - self.max_padding\n",
    "#         right_bound = idx + pattern_len + self.max_padding\n",
    "#         if left_bound < 0: left_bound = 0\n",
    "#         if right_bound >= string_len : right_bound = string_len\n",
    "            \n",
    "#         if idx - left_bound <= 1: left_bound = idx\n",
    "#         if right_bound - (idx + pattern_len) == 1: right_bound = right_bound - 1\n",
    "#         return string[left_bound : right_bound]\n",
    "    \n",
    "#     def search_answer(self):\n",
    "#         cleaned_content = self.clean_html(self.google_crawl(self.short_query))\n",
    "#         for (m) in re.findall(self.short_pattern, cleaned_content):\n",
    "#             ans = m.strip().lower()\n",
    "# #             print(ans)\n",
    "#             if ans in self.options:\n",
    "#                 return (self.options.index(ans), ans)\n",
    "#                 break\n",
    "    \n",
    "#     def search_fast_answer(self):\n",
    "#         raw_html = self.google_crawl(self.raw_query).lower()\n",
    "#         clean_html = self.clean_html(raw_html)\n",
    "#         for x in self.options:\n",
    "#             if clean_html.find(x) > 0:\n",
    "#                 return (self.options.index(x), x)\n",
    "        \n",
    "#     def google_crawl(self, query = None):\n",
    "#         if query == None: query = self.short_query\n",
    "#         self.link = \"https://www.google.com/search?q=\" + quote(query)  + '&ie=utf8&oe=utf8' # + '&lr=lang_zh-TW'\n",
    "#         req = request.Request(self.link, headers = {'User-Agent' : \"Magic Browser\"})\n",
    "#         raw = html.unescape(request.urlopen(req).read().decode('utf-8'))\n",
    "#         return raw\n",
    "    \n",
    "#     def clean_html(self, raw_html = None):\n",
    "#         if raw_html == None: raw_html = self.google_crawl(self.short_query)\n",
    "#         clean_html = re.sub(re.compile(r'(<br?>)|(</br?>)|\\n|\\r|\\s'), '', raw_html)\n",
    "#         return clean_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[failure]2016-08-02-07-28-35.txt:[1]\n",
      "[failure]2016-08-07-01-23-23.txt:[1]\n",
      "[failure]2016-08-07-12-46-48.txt:[1]\n",
      "[failure]2016-08-07-12-46-48.txt:[5]\n",
      "[failure]2016-08-04-01-56-08.txt:[1]\n",
      "[failure]2016-08-04-01-56-08.txt:[5]\n",
      "[failure]2016-08-03-19-43-20.txt:[1]\n",
      "[failure]2016-08-03-19-43-20.txt:[3]\n",
      "0.9207920792079208\n",
      "[failure]2016-08-03-18-41-11.txt:[2]\n",
      "[failure]2016-08-03-18-41-11.txt:[4]\n",
      "[failure]2016-08-05-03-49-29.txt:[5]\n",
      "[failure]2016-08-05-16-15-09.txt:[3]\n",
      "[failure]2016-08-01-13-49-09.txt:[1]\n",
      "[failure]2016-08-05-17-17-17.txt:[1]\n",
      "[failure]2016-08-07-06-34-01.txt:[1]\n",
      "[failure]2016-08-07-06-34-01.txt:[3]\n",
      "0.9203980099502488\n",
      "[failure]2016-08-05-06-55-54.txt:[3]\n",
      "[failure]2016-08-05-07-58-03.txt:[3]\n",
      "[failure]2016-08-01-10-42-46.txt:[3]\n",
      "[failure]2016-08-05-14-10-51.txt:[2]\n",
      "[failure]2016-08-05-22-27-57.txt:[1]\n",
      "[failure]2016-08-07-15-53-08.txt:[1]\n",
      "[failure]2016-08-02-12-39-15.txt:[5]\n",
      "0.9235880398671097\n",
      "[failure]2016-08-04-14-21-42.txt:[5]\n",
      "[failure]2016-08-06-13-59-56.txt:[2]\n",
      "[failure]2016-08-04-05-02-34.txt:[1]\n",
      "[failure]2016-08-02-06-26-27.txt:[1]\n",
      "[failure]2016-08-03-22-49-45.txt:[5]\n",
      "0.9301745635910225\n",
      "[failure]2016-08-02-03-20-07.txt:[2]\n",
      "[failure]2016-08-05-23-30-05.txt:[5]\n",
      "[failure]2016-08-06-04-40-43.txt:[1]\n",
      "[failure]2016-08-02-11-37-06.txt:[1]\n",
      "[failure]2016-08-03-23-51-53.txt:[3]\n",
      "0.9341317365269461\n",
      "[failure]2016-08-05-05-53-48.txt:[5]\n",
      "[failure]2016-08-03-14-32-37.txt:[1]\n",
      "[failure]2016-08-01-06-34-12.txt:[5]\n",
      "[failure]2016-08-01-09-40-39.txt:[3]\n",
      "[failure]2016-08-01-03-27-45.txt:[3]\n",
      "[failure]2016-08-01-03-27-45.txt:[5]\n",
      "0.935108153078203\n",
      "[failure]2016-08-02-05-24-21.txt:[3]\n",
      "[failure]2016-07-31-17-06-20.txt:[5]\n",
      "[failure]2016-08-04-12-17-27.txt:[4]\n",
      "[failure]2016-08-04-12-17-27.txt:[3]\n",
      "[failure]2016-08-05-00-43-00.txt:[3]\n",
      "[failure]2016-08-05-00-43-00.txt:[5]\n",
      "[failure]2016-07-31-15-02-02.txt:[4]\n",
      "[failure]2016-07-31-15-02-02.txt:[3]\n",
      "0.9329529243937232\n",
      "[failure]2016-08-03-03-09-08.txt:[2]\n",
      "[failure]2016-08-07-10-42-31.txt:[3]\n",
      "[failure]2016-08-07-09-40-23.txt:[5]\n",
      "[failure]2016-08-07-05-31-54.txt:[2]\n",
      "[failure]2016-08-07-05-31-54.txt:[4]\n",
      "[failure]2016-08-05-18-19-25.txt:[1]\n",
      "0.933832709113608\n",
      "[failure]2016-08-01-08-38-29.txt:[5]\n",
      "[failure]2016-08-02-17-49-56.txt:[1]\n",
      "0.934052757793765\n"
     ]
    }
   ],
   "source": [
    "path = 'question_samples/'\n",
    "sample_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "i, j = 0, 0\n",
    "for file in sample_files:\n",
    "    with codecs.open(path + file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.find('[END]') >= 0:\n",
    "                crawler = GoogleCrawler(line)\n",
    "                try:\n",
    "                    ans = crawler.search_fast_answer()\n",
    "                    if ans == None:\n",
    "                        print('[failure]{}:{}'.format(file, line[:3]))\n",
    "                        j += 1\n",
    "#                     else:\n",
    "#                         print(ans)\n",
    "                except:\n",
    "                    print('[exception]:{}:{}'.format(file, line[:3]))\n",
    "                    j += 1\n",
    "                i += 1\n",
    "                if i % 100 == 1: print(1. - float(j) / float(i))\n",
    "print(1. - float(j) / float(i))\n",
    "                \n",
    "# for i in range(20):\n",
    "#     print(i)\n",
    "#     question = samples[i]\n",
    "#     crawler = GoogleCrawler(question)\n",
    "#     print('{}\\n{}'.format(crawler.short_query, crawler.short_pattern))    \n",
    "#     print(crawler.search_fast_answer())\n",
    "#     print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "[failure]2016-08-02-07-28-35.txt:[1]\n",
    "[failure]2016-08-07-01-23-23.txt:[1]\n",
    "[failure]2016-08-07-12-46-48.txt:[1]\n",
    "[failure]2016-08-07-12-46-48.txt:[5]\n",
    "[failure]2016-08-04-01-56-08.txt:[1]\n",
    "[failure]2016-08-04-01-56-08.txt:[5]\n",
    "[failure]2016-08-03-19-43-20.txt:[1]\n",
    "[failure]2016-08-03-19-43-20.txt:[3]\n",
    "[failure]2016-08-03-18-41-11.txt:[2]\n",
    "[failure]2016-08-03-18-41-11.txt:[4]\n",
    "[failure]2016-08-05-03-49-29.txt:[5]\n",
    "[failure]2016-08-05-16-15-09.txt:[3]\n",
    "[failure]2016-08-01-13-49-09.txt:[1]\n",
    "[failure]2016-08-05-17-17-17.txt:[1]\n",
    "[failure]2016-08-07-06-34-01.txt:[1]\n",
    "[failure]2016-08-07-06-34-01.txt:[3]\n",
    "[failure]2016-08-05-06-55-54.txt:[3]\n",
    "[failure]2016-08-05-07-58-03.txt:[3]\n",
    "[failure]2016-08-01-10-42-46.txt:[3]\n",
    "[failure]2016-08-05-14-10-51.txt:[2]\n",
    "[failure]2016-08-05-22-27-57.txt:[1]\n",
    "[failure]2016-08-07-15-53-08.txt:[1]\n",
    "[failure]2016-08-02-12-39-15.txt:[5]\n",
    "[failure]2016-08-04-14-21-42.txt:[5]\n",
    "[failure]2016-08-06-13-59-56.txt:[2]\n",
    "[failure]2016-08-04-05-02-34.txt:[1]\n",
    "[failure]2016-08-02-06-26-27.txt:[1]\n",
    "[failure]2016-08-03-22-49-45.txt:[5]\n",
    "[failure]2016-08-02-03-20-07.txt:[2]\n",
    "[failure]2016-08-05-23-30-05.txt:[5]\n",
    "[failure]2016-08-06-04-40-43.txt:[1]\n",
    "[failure]2016-08-02-11-37-06.txt:[1]\n",
    "[failure]2016-08-03-23-51-53.txt:[3]\n",
    "[failure]2016-08-05-05-53-48.txt:[5]\n",
    "[failure]2016-08-03-14-32-37.txt:[1]\n",
    "[failure]2016-08-01-06-34-12.txt:[5]\n",
    "[failure]2016-08-01-09-40-39.txt:[3]\n",
    "[failure]2016-08-01-03-27-45.txt:[3]\n",
    "[failure]2016-08-01-03-27-45.txt:[5]\n",
    "[failure]2016-08-02-05-24-21.txt:[3]\n",
    "[failure]2016-07-31-17-06-20.txt:[5]\n",
    "[failure]2016-08-04-12-17-27.txt:[4]\n",
    "[failure]2016-08-04-12-17-27.txt:[3]\n",
    "[failure]2016-08-05-00-43-00.txt:[3]\n",
    "[failure]2016-08-05-00-43-00.txt:[5]\n",
    "[failure]2016-07-31-15-02-02.txt:[4]\n",
    "[failure]2016-07-31-15-02-02.txt:[3]\n",
    "[failure]2016-08-03-03-09-08.txt:[2]\n",
    "[failure]2016-08-07-10-42-31.txt:[3]\n",
    "[failure]2016-08-07-09-40-23.txt:[5]\n",
    "[failure]2016-08-07-05-31-54.txt:[2]\n",
    "[failure]2016-08-07-05-31-54.txt:[4]\n",
    "[failure]2016-08-05-18-19-25.txt:[1]\n",
    "[failure]2016-08-01-08-38-29.txt:[5]\n",
    "[failure]2016-08-02-17-49-56.txt:[1]\n",
    "0.934052757793765\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename/no: 2016-08-02-07-28-35.txt/1,\tqLen: 107,\tqNum: 1\n",
      "filename/no: 2016-08-07-01-23-23.txt/1,\tqLen: 141,\tqNum: 1\n",
      "filename/no: 2016-08-07-12-46-48.txt/1,\tqLen: 36,\tqNum: 1\n",
      "filename/no: 2016-08-07-12-46-48.txt/5,\tqLen: 50,\tqNum: 1\n",
      "filename/no: 2016-08-04-01-56-08.txt/1,\tqLen: 36,\tqNum: 1\n",
      "filename/no: 2016-08-04-01-56-08.txt/5,\tqLen: 50,\tqNum: 1\n",
      "filename/no: 2016-08-03-19-43-20.txt/1,\tqLen: 187,\tqNum: 1\n",
      "filename/no: 2016-08-03-19-43-20.txt/3,\tqLen: 183,\tqNum: 1\n",
      "filename/no: 2016-08-03-18-41-11.txt/2,\tqLen: 145,\tqNum: 1\n",
      "filename/no: 2016-08-03-18-41-11.txt/4,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-05-03-49-29.txt/5,\tqLen: 39,\tqNum: 1\n",
      "filename/no: 2016-08-05-16-15-09.txt/3,\tqLen: 40,\tqNum: 1\n",
      "filename/no: 2016-08-01-13-49-09.txt/1,\tqLen: 105,\tqNum: 1\n",
      "filename/no: 2016-08-05-17-17-17.txt/1,\tqLen: 95,\tqNum: 1\n",
      "filename/no: 2016-08-07-06-34-01.txt/1,\tqLen: 187,\tqNum: 1\n",
      "filename/no: 2016-08-07-06-34-01.txt/3,\tqLen: 183,\tqNum: 1\n",
      "filename/no: 2016-08-05-06-55-54.txt/3,\tqLen: 106,\tqNum: 1\n",
      "filename/no: 2016-08-05-07-58-03.txt/3,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-01-10-42-46.txt/3,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-05-14-10-51.txt/2,\tqLen: 53,\tqNum: 1\n",
      "filename/no: 2016-08-05-22-27-57.txt/1,\tqLen: 66,\tqNum: 1\n",
      "filename/no: 2016-08-07-15-53-08.txt/1,\tqLen: 37,\tqNum: 1\n",
      "filename/no: 2016-08-02-12-39-15.txt/5,\tqLen: 171,\tqNum: 1\n",
      "filename/no: 2016-08-04-14-21-42.txt/5,\tqLen: 42,\tqNum: 1\n",
      "filename/no: 2016-08-06-13-59-56.txt/2,\tqLen: 47,\tqNum: 1\n",
      "filename/no: 2016-08-04-05-02-34.txt/1,\tqLen: 37,\tqNum: 1\n",
      "filename/no: 2016-08-02-06-26-27.txt/1,\tqLen: 95,\tqNum: 1\n",
      "filename/no: 2016-08-03-22-49-45.txt/5,\tqLen: 167,\tqNum: 1\n",
      "filename/no: 2016-08-02-03-20-07.txt/2,\tqLen: 53,\tqNum: 1\n",
      "filename/no: 2016-08-05-23-30-05.txt/5,\tqLen: 171,\tqNum: 1\n",
      "filename/no: 2016-08-06-04-40-43.txt/1,\tqLen: 58,\tqNum: 1\n",
      "filename/no: 2016-08-02-11-37-06.txt/1,\tqLen: 66,\tqNum: 1\n",
      "filename/no: 2016-08-03-23-51-53.txt/3,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-05-05-53-48.txt/5,\tqLen: 33,\tqNum: 2\n",
      "filename/no: 2016-08-03-14-32-37.txt/1,\tqLen: 141,\tqNum: 1\n",
      "filename/no: 2016-08-01-06-34-12.txt/5,\tqLen: 39,\tqNum: 1\n",
      "filename/no: 2016-08-01-09-40-39.txt/3,\tqLen: 106,\tqNum: 1\n",
      "filename/no: 2016-08-01-03-27-45.txt/3,\tqLen: 42,\tqNum: 1\n",
      "filename/no: 2016-08-01-03-27-45.txt/5,\tqLen: 159,\tqNum: 1\n",
      "filename/no: 2016-08-02-05-24-21.txt/3,\tqLen: 40,\tqNum: 1\n",
      "filename/no: 2016-07-31-17-06-20.txt/5,\tqLen: 42,\tqNum: 1\n",
      "filename/no: 2016-08-04-12-17-27.txt/4,\tqLen: 32,\tqNum: 3\n",
      "filename/no: 2016-08-04-12-17-27.txt/3,\tqLen: 41,\tqNum: 1\n",
      "filename/no: 2016-08-05-00-43-00.txt/3,\tqLen: 42,\tqNum: 1\n",
      "filename/no: 2016-08-05-00-43-00.txt/5,\tqLen: 159,\tqNum: 1\n",
      "filename/no: 2016-07-31-15-02-02.txt/4,\tqLen: 32,\tqNum: 3\n",
      "filename/no: 2016-07-31-15-02-02.txt/3,\tqLen: 41,\tqNum: 1\n",
      "filename/no: 2016-08-03-03-09-08.txt/2,\tqLen: 47,\tqNum: 1\n",
      "filename/no: 2016-08-07-10-42-31.txt/3,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-07-09-40-23.txt/5,\tqLen: 167,\tqNum: 1\n",
      "filename/no: 2016-08-07-05-31-54.txt/2,\tqLen: 145,\tqNum: 1\n",
      "filename/no: 2016-08-07-05-31-54.txt/4,\tqLen: 35,\tqNum: 1\n",
      "filename/no: 2016-08-05-18-19-25.txt/1,\tqLen: 107,\tqNum: 1\n",
      "filename/no: 2016-08-01-08-38-29.txt/5,\tqLen: 33,\tqNum: 2\n",
      "filename/no: 2016-08-02-17-49-56.txt/1,\tqLen: 58,\tqNum: 1\n"
     ]
    }
   ],
   "source": [
    "with open('question_samples/failure_log.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        filename, no = re.findall(r'\\[failure\\](.*):\\[(\\d+)\\]', line)[0]\n",
    "        with codecs.open('question_samples/' + filename, 'r', 'utf-8') as g:\n",
    "            for s in g:\n",
    "                try:\n",
    "                    crawler = GoogleCrawler(s)\n",
    "                except:\n",
    "                    continue\n",
    "                if int(no) == int(crawler.no):\n",
    "                    print('filename/no: {}/{},\\tqLen: {},\\tqNum: {}'.format(filename, no, len(crawler.raw_query), len(re.findall(r'\\*', crawler.raw_query))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*水利局副局長彭紹博表示，該工程是經濟部水利署所建，但因工程已經完工，市府針對此狀況，正積極尋求補救，希望在兩岸闢建公園，但因地價太高，還在努力中\n",
      "['保護區', '台南市', '大院', '銅鑼', '真愛碼頭']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, '台南市')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'question_samples/'\n",
    "filename = '2016-07-31-15-02-02.txt'\n",
    "no = 5\n",
    "def get_sample(file_name, no_sample):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            for (no, content, a, b, c, d, e) in re.findall(r'\\[(\\d+)\\](.*)### a:(.*), b:(.*), c:(.*), d:(.*), e:(.*)\\[END\\]', line):\n",
    "                if int(no) == int(no_sample):\n",
    "                    return line\n",
    "            \n",
    "crawler = GoogleCrawler(get_sample(path + filename, no))\n",
    "print(crawler.raw_query)\n",
    "print(crawler.options)\n",
    "crawler.search_fast_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crawler.raw_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insert', 0, 0, 0, 16401),\n",
       " ('equal', 0, 36, 16401, 16437),\n",
       " ('insert', 36, 36, 16437, 47730)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "sm = difflib.SequenceMatcher(None, a=src, b=tar )\n",
    "sm.get_opcodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "a = '他說 然後擦*前有刮鬍子  他覺得這款保濕露對刮鬍子後的刺激感有舒緩功'\n",
    "# a.find('*')\n",
    "# m = list(re.finditer(r'\\*',a))\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('因為地處*的高點--號稱\"全*最高的溫泉\",又有\"*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
